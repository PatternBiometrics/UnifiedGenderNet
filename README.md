# UnifiedGenderNet  
[![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)  [![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE-CODE.md)  [![Status](https://img.shields.io/badge/status-under--review-yellow.svg)](#)  

A **unified** deep learning model for gender prediction that works with **either a hand or a face** image at inference time.  

It includes two architectural variants:
- **UMCC** (Unified Modality-Conditioned Classifier)  
- **MAG** (Modality-Aware Gated network)  

‚ö†Ô∏è **Disclaimer**: This repository accompanies a manuscript currently under peer review.  
To avoid premature disclosure of unpublished material, only baseline-level results are shown here.  
Full ablations, fairness analyses, and cross-dataset evaluations will be released upon publication.

---

## 1. Quick Start

### Clone the repository  

```bash
git clone https://github.com/PatternBiometrics/UnifiedGenderNet.git
cd UnifiedGenderNet
```

### Create and activate a virtual environment

```bash
python -m venv .venv
source .venv/bin/activate    # on Linux/macOS
# .venv\Scripts\activate     # on Windows PowerShell
```

### Install dependencies

```bash
pip install -r requirements.txt
```

### Download pretrained artifacts

```bash
python scripts/download_checkpoints.py
python scripts/download_metrics_csv.py
```

---

## 2. Dataset

We release a **derived subset of HaGRID** with aligned face‚Äìhand crops.  
Every record pairs a cropped hand with the same user‚Äôs face and comes with audited demographic labels:

| Column            | Description                               |
|-------------------|-------------------------------------------|
| `id`              | Unique sample ID                          |
| `user_id`         | Subject identifier (across samples)       |
| `age`             | Age bucket                                |
| `gender`          | Male / Female                             |
| `race`            | Skin-tone class                           |
| `labels`          | HaGRID gesture labels (stop, stop_inverted, palm) |
| `hand_image_name` | Filename of the cropped hand              |
| `face_image_name` | Filename of the cropped face              |

### Data distribution

* **`aligned_dataset.zip`** ‚Äì cropped & aligned images  
* **`train.csv` ¬∑ `val.csv` ¬∑ `test.csv`** ‚Äì metadata splits  

### Data structure

```
data/
‚îî‚îÄ‚îÄ Shared_Derived_HaGRID_unified_Model_For_Sex_Prediction/
    ‚îú‚îÄ‚îÄ train.csv
    ‚îú‚îÄ‚îÄ val.csv
    ‚îú‚îÄ‚îÄ test.csv
    ‚îî‚îÄ‚îÄ images/
        ‚îî‚îÄ‚îÄ content/
            ‚îî‚îÄ‚îÄ aligned_dataset/
                ‚îú‚îÄ‚îÄ faces/
                ‚îÇ   ‚îú‚îÄ‚îÄ F_<uuid>.jpg
                ‚îÇ   ‚îî‚îÄ‚îÄ ‚Ä¶
                ‚îî‚îÄ‚îÄ hands/
                    ‚îú‚îÄ‚îÄ H_<uuid>.jpg
                    ‚îî‚îÄ‚îÄ ‚Ä¶
```

* **Face crops** in `faces/` start with `F_`  
* **Hand crops** in `hands/` start with `H_`  
* The UUID suffix is identical for the paired images  

> Example pair:  
> `faces/F_fe7d7a58-cc64-4755-849a-55970b08b75a.jpg`  
> `hands/H_fe7d7a58-cc64-4755-849a-55970b08b75a.jpg`

### Minimal usage example

```python
import pandas as pd
from pathlib import Path

root = Path("data/Shared_Derived_HaGRID_unified_Model_For_Sex_Prediction")
df   = pd.read_csv(root / "train.csv")
print(f"{len(df):,} training samples")
print(df.head())

img_root = root / "images" / "content" / "aligned_dataset"
hand_path = img_root / "hands" / df.loc[0, "hand_image_name"]
face_path = img_root / "faces" / df.loc[0, "face_image_name"]
print(hand_path, face_path, sep="\n")
```

---

## 3. Checkpoints

* Pretrained model weights (UMCC / MAG, EfficientNetV2-S) are hosted on Google Drive.  
* Fetch with:  
  ```bash
  python scripts/download_checkpoints.py
  ```  
Perfect idea ‚úÖ ‚Äî adding a **legend/cheat sheet** into the README will make it crystal clear for reviewers and colleagues what those cryptic tags mean. Here‚Äôs the updated section you can **drop into your README.md** (just after *Checkpoints*):

---

## 3. Checkpoints

* Pretrained model weights (UMCC / MAG, EfficientNetV2-S) are hosted on Google Drive.
* Fetch with:

  ```bash
  python scripts/download_checkpoints.py
  ```

### Tag naming convention

Checkpoint tags follow the pattern:

```
<backbone>.<pretrain>_F#_A#_L#_Z#
```

| Code             | Meaning                                                                                           |
| ---------------- | ------------------------------------------------------------------------------------------------- |
| **Backbone**     | e.g. `tf_efficientnetv2_s.in1k` ‚Üí EfficientNetV2-S pretrained on ImageNet-1k                      |
| **F0 / F1**      | **Flag conditioning** (UMCC only): `F0` = flag OFF, `F1` = flag ON (hand = \[1,0], face = \[0,1]) |
| **Af / An**      | **Augmentation**: `Af` = Full augmentation (flips, color jitter, rotation, etc.), `An` = None     |
| **Lb / Lf / Lw** | **Loss function**: `Lb` = Binary Cross-Entropy, `Lf` = Focal Loss, `Lw` = Weighted BCE            |
| **Z0 / Z1 / ‚Ä¶**  | **Freezing policy**: `Z0` = no freezing (all layers trainable), `Z1` = partial freeze, etc.       |



‚Üí EfficientNetV2-S backbone, pretrained on ImageNet-1k, **flag OFF**, **full augmentation**, **focal loss**, **no freezing**.


üìå **Best checkpoint**: The highest-scoring model (Balanced Accuracy ‚âà 0.91, ROC‚ÄìAUC ‚âà 0.97) is stored as
[checkpoints/1752668630.pt](https://drive.google.com/file/d/1yK36dx8mdG5dvxZNNc50pHoALl77M9jX/view?usp=sharing) .
This corresponds to the configuration tag:

```
tf_efficientnetv2_s.in1k_F0_Af_Lf_Z0
```

‚Üí EfficientNetV2-S backbone, pretrained on ImageNet-1k, flag OFF, full augmentation, focal loss, no freezing.

---

## 4. Metrics

* CSV file: `metrics/final_metrics_with_full_configurations.csv`  
* Contains all configs and evaluation metrics.  


---

## 5. Results (Preview Only)

Baseline (UMCC, EfficientNetV2-S, full augmentation, BCE, no freezing):

* Validation balanced accuracy ‚âà **0.91**  
* Test accuracy ‚âà **91%**  
* Test ROC‚ÄìAUC ‚âà **0.97**  

üìå Full results (ablations, fairness, cross-dataset) will be added once the paper is accepted.

---

## 6. Ethics & Licensing

* Labels = **apparent gender**, used only for evaluation  
* Demographics used strictly for fairness assessment  
* **Source** ‚Äì Derived from HaGRID (public licence with attribution & conditions)  
* **Derived data** ‚Äì redistributed under same licence  
* **Code** ‚Äì MIT (`LICENSE-CODE.md`)  

---

## 7. Dependencies

* Python ‚â• 3.9  
* `torch`, `torchvision`, `timm`, `pandas`, `gdown`, `tabulate`, `typer`, `rich`

---


## 8. Acknowledgements

* Huge thanks to the original **HaGRID** authors for releasing their dataset.  
* Gratitude to the open-source community for providing tools that enabled this derivative work.
